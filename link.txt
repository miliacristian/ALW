link documento word condiviso:
 https://drive.google.com/open?id=1Zjj1wYKzFirIboGaJ9OLSTUKZlPfEqawaxehFHtSugk

passare alla funzione load dataset i booleani standardized e normalized per caricare un dataset e renderlo normalizato/standardizzato
ha senso rendere un dataset sia normalizzato sia standardizzato?

i warning non sono spariti?sono attivi i warning ignore?
def fxn()://devono stare necessariamente in 2 file??
    warnings.warn("Undefined metric", UndefinedMetricWarning)


with warnings.catch_warnings()://devono stare necessariamente in 2 file??
    warnings.simplefilter("ignore")
    fxn()

http://scikit-learn.org/0.18/auto_examples/missing_values.html #missign value scikit learn
http://scikit-learn.org/dev/modules/impute.html
http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html
https://machinelearningmastery.com/handle-missing-data-python/ #master machine learning

https://www.datacamp.com/community/tutorials/random-forests-classifier-python#findingfeatures link per vedere l'importanza delle features

in teoria se non esistono variabili da predirre si può fare regressione su qualsiasi variabile continua(sfruttando le altre?)


    -   training --> testing su ogni caso
    -   i casi sono:
        -   dataset originali (senza norm e stand, solo norm, solo stand).
        -   dataset con NaN (rimuovi righe con NaN, stategia media, mediana, moda)
            dataset con NaN generati con percentuali di NaN pari a 0.05, 0.1, 0.15.

    Che radar plot creiamo? esempio, RF al variare della strategia usata per il filling?

    problema: dataset piccoli (seed e zoo), strategia eliminate_row da problemi con KNN (Fold con pochissimi elementi)

    balance --> all    FATTO
    eye --> mean, median, eliminate_row
    page --> mean, median, eliminate_row
    seed --> mean, median  FATTO
    tris --> mode, eliminate_row
    zoo --> mode    FATTO

    airfoil --> mean, median, eliminate_row
    auto --> index 0, 5, 6 mode + mean/median, eliminate_row
    power_plant --> mean, median, eliminate_row
    compressive --> index 7 mode + mean/median, eliminate_row
    energy --> index 4, 5, 6, 7 + mean/median, eliminate_row

   -    REGRESSIONE:
        -   aggiustare dataset  energy (sembra fattibile. multilabel ?, anche se supportato dalle metriche sembra)
                                com (ok),
                                auto (ok, attenzione valori mancanti),
                                airfull (ok)
        -   scegliere modelli:  SVR (idem SVC per regressione),
                                KNeighborsRegressor (KNN per regressione),
                                RadiusNeighborsRegressor (idem KNN ma non vede i K elementi più vicini,
                                    ma quelli che distano meno di un certo raggio),
                                DecisionTreeRegressor (CART for regressione),
                                MLPRegressor (reti neurali per regressione),
                                RandomForestRegressor (RF per regressione)
        -   scegliere metriche: 'explained_variance' (tra -inf e 1),
                                'neg_mean_absolute_error' (tra -inf e 0),
                                'neg_mean_squared_error' (tra -inf e 0),
                                'neg_mean_squared_log_error',
                                'neg_median_absolute_error',
                                'r2' (tra -inf e 0)

#TODO
	- link.txt serve per la relazione
	- scrivere relazione
	- preparare la presentazione


	- commentare funzioni da riga 10 a 79 in file main.py -> ema
	- completare docstring(mancano i commenti sui parametri e sui valori di ritorno) da riga 1 fino a riga 91 nel file scoringUtils.py -> ema

	- cosa fa e dove è utilizzata la funzione build_models riga 648 file training?
	- eliminare p.py
	- eliminare main scoringUtils.py
	- eliminare SVC_default_training riga 102 file training.py
	- eliminare cartella model_setting_test

	- fare training dataset regression
		airfoil 1500 -> ema
		auto 400 -> ema
		energy 800 -> ema
		plant 9500 -> cri
		compressive 1000 -> ema